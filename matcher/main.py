"""
Main entry point for Student Analysis Agent
With vLLM and SentenceTransformer embeddings support
"""
import json
from typing import Dict, Any, Optional

from agent import create_agent, GraphState, analyze_with_local_model
from knowledge_graph import KnowledgeGraphManager, TopicMatcher, create_topic_links
from vllm_client import VLLMConfig, VLLMClient, get_vllm_launch_command, print_vllm_setup_instructions


def run_analysis(
    model_output: Optional[str] = None,
    knowledge_graph_json: Optional[str] = None,
    user_prompt: Optional[str] = None,
    # vLLM options (used if model_output is None)
    use_vllm: bool = False,
    question: Optional[str] = None,
    correct_answer: Optional[str] = None,
    student_answer: Optional[str] = None,
    student_explanation: Optional[str] = None,
    vllm_host: str = "localhost",
    vllm_port: int = 8000,
    model_path: str = "models/supervised_v1",
    adapter_path: str = "models/adapter_v1",
    # Embedding options
    use_embeddings: bool = True,
    embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
) -> Dict[str, Any]:
    """
    Run the student analysis agent
    
    Args:
        model_output: Pre-generated model output (with JSON analysis)
        knowledge_graph_json: JSON string of the knowledge graph
        user_prompt: Original user prompt (optional)
        use_vllm: Whether to use vLLM for analysis (if model_output not provided)
        question: Question asked (for vLLM)
        correct_answer: Correct answer (for vLLM)
        student_answer: Student's answer (for vLLM)
        student_explanation: Student's explanation (for vLLM)
        vllm_host: vLLM server host
        vllm_port: vLLM server port
        model_path: Path to the base model
        adapter_path: Path to the LoRA adapter
        use_embeddings: Whether to use SentenceTransformer embeddings
        embedding_model: SentenceTransformer model name
    
    Returns:
        Dictionary with analysis results and recommendations
    """
    # Create and run the agent
    agent = create_agent()
    
    initial_state: GraphState = {
        "user_prompt": user_prompt or "",
        "question": question,
        "correct_answer": correct_answer,
        "student_answer": student_answer,
        "student_explanation": student_explanation,
        "model_output": model_output,
        "knowledge_graph_json": knowledge_graph_json,
        "parsed_response": None,
        "analysis_result": None,
        "kg_manager": None,
        "embedding_manager": None,
        "semantic_matcher": None,
        "topic_matches": [],
        "topic_links": None,
        "semantic_matches": None,
        "final_recommendations": None,
        "messages": [],
        "processing_steps": [],
        "errors": [],
        "use_vllm": use_vllm,
        "vllm_config": {
            "host": vllm_host,
            "port": vllm_port,
            "model_path": model_path,
            "adapter_path": adapter_path
        },
        "use_embeddings": use_embeddings,
        "embedding_model": embedding_model
    }
    
    # Run the graph
    result = agent.invoke(initial_state)
    
    return {
        "recommendations": result.get("final_recommendations"),
        "processing_steps": result.get("processing_steps"),
        "errors": result.get("errors"),
        "analysis_result": result.get("analysis_result"),
        "topic_matches": result.get("topic_matches"),
        "semantic_matches": result.get("semantic_matches")
    }


def main():
    """Example usage with the provided data"""
    
    # Print vLLM setup instructions
    print_vllm_setup_instructions()
    
    # Example model output (from your data)
    model_output = """<think>–ê–Ω–∞–ª—ñ–∑—É—é –≤—ñ–¥–ø–æ–≤—ñ–¥—å —É—á–Ω—è –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è. –£—á–µ–Ω—å –¥–æ–ø—É—Å—Ç–∏–≤ –ø–æ–º–∏–ª–∫—É —Ç–∏–ø—É 'None'. –°–ª–∞–±–∫—ñ –∫–æ–Ω—Ü–µ–ø—Ü—ñ—ó: –ü—Ä–∞–≤–∏–ª–∞ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—Ä—è–º–æ—ó –º–æ–≤–∏ –≤ –Ω–µ–ø—Ä—è–º—É, –æ—Å–æ–±–ª–∏–≤–æ –¥–ª—è —Å–ø–æ–Ω—É–∫–∞–ª—å–Ω–∏—Ö —Ä–µ—á–µ–Ω—å., –†–æ–∑—É–º—ñ–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ —Ä–µ—á–µ–Ω—å –∑ –Ω–µ–ø—Ä—è–º–æ—é –º–æ–≤–æ—é. –í—ñ–¥–ø–æ–≤—ñ–¥—å –ø–æ—Ç—Ä–µ–±—É—î —Å—É—Ç—Ç—î–≤–æ–≥–æ –¥–æ–æ–ø—Ä–∞—Ü—é–≤–∞–Ω–Ω—è.</think>
{
  "is_correct": false,
  "score": 0,
  "summary": "–í—ñ–¥–ø–æ–≤—ñ–¥—å —É—á–Ω—è –Ω–µ–≤—ñ–¥–æ–º–∞, —Ç–æ–º—É –æ—Ü—ñ–Ω–∏—Ç–∏ —ó—ó –Ω–µ–º–æ–∂–ª–∏–≤–æ. –£—á–µ–Ω—å –Ω–µ –Ω–∞–¥–∞–≤ –∂–æ–¥–Ω–æ—ó –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è.",
  "mistakes": [],
  "weak_terms": [
    "–ù–µ–ø—Ä—è–º–∞ –º–æ–≤–∞",
    "–°–ø–æ–Ω—É–∫–∞–ª—å–Ω–µ —Ä–µ—á–µ–Ω–Ω—è",
    "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è —Ä–µ—á–µ–Ω—å"
  ],
  "weak_concepts": [
    "–ü—Ä–∞–≤–∏–ª–∞ –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—Ä—è–º–æ—ó –º–æ–≤–∏ –≤ –Ω–µ–ø—Ä—è–º—É, –æ—Å–æ–±–ª–∏–≤–æ –¥–ª—è —Å–ø–æ–Ω—É–∫–∞–ª—å–Ω–∏—Ö —Ä–µ—á–µ–Ω—å.",
    "–†–æ–∑—É–º—ñ–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ —Ä–µ—á–µ–Ω—å –∑ –Ω–µ–ø—Ä—è–º–æ—é –º–æ–≤–æ—é."
  ],
  "weak_skills": [
    "–ê–Ω–∞–ª—ñ–∑ —Ä–µ—á–µ–Ω—å",
    "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—è —Ä–µ—á–µ–Ω—å –∑ –ø—Ä—è–º–æ—ó –º–æ–≤–∏ –≤ –Ω–µ–ø—Ä—è–º—É",
    "–ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –≥—Ä–∞–º–∞—Ç–∏–∫–∏"
  ],
  "correct_aspects": [],
  "suggested_topics": [
    "¬ß 5. –ù–µ–ø—Ä—è–º–∞ –º–æ–≤–∞. –ó–∞–º—ñ–Ω–∞ –ø—Ä—è–º–æ—ó –º–æ–≤–∏ –Ω–µ–ø—Ä—è–º–æ—é",
    "–¢–∏–ø–∏ —Ä–µ—á–µ–Ω—å –∑–∞ –º–µ—Ç–æ—é –≤–∏—Å–ª–æ–≤–ª—é–≤–∞–Ω–Ω—è (–∑–æ–∫—Ä–µ–º–∞, —Å–ø–æ–Ω—É–∫–∞–ª—å–Ω—ñ)",
    "–°–ø–æ–ª—É—á–Ω–∏–∫–∏ –≤ —Å–∫–ª–∞–¥–Ω–æ–ø—ñ–¥—Ä—è–¥–Ω–∏—Ö —Ä–µ—á–µ–Ω–Ω—è—Ö"
  ],
  "explanation": "–ü—Ä–∞–≤–∏–ª—å–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è –≤–∏–º–∞–≥–∞—î –∑–Ω–∞–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—Ä—è–º–æ—ó –º–æ–≤–∏ –≤ –Ω–µ–ø—Ä—è–º—É, –æ—Å–æ–±–ª–∏–≤–æ –∫–æ–ª–∏ –π–¥–µ—Ç—å—Å—è –ø—Ä–æ —Å–ø–æ–Ω—É–∫–∞–ª—å–Ω—ñ —Ä–µ—á–µ–Ω–Ω—è. –£ —Ç–∞–∫–æ–º—É –≤–∏–ø–∞–¥–∫—É, –ø—Ä—è–º–∞ –º–æ–≤–∞, —â–æ –≤–∏—Ä–∞–∂–∞—î –ø—Ä–æ—Ö–∞–Ω–Ω—è –∞–±–æ –Ω–∞–∫–∞–∑, –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î—Ç—å—Å—è –Ω–∞ –ø—ñ–¥—Ä—è–¥–Ω–µ —Ä–µ—á–µ–Ω–Ω—è, —â–æ –≤–≤–æ–¥–∏—Ç—å—Å—è —Å–ø–æ–ª—É—á–Ω–∏–∫–æ–º '—â–æ–±'. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, —Ä–µ—á–µ–Ω–Ω—è ¬´–ü—Ä–∏–Ω–µ—Å–∏, –±—É–¥—å –ª–∞—Å–∫–∞, –∫–≤—ñ—Ç–∏¬ª, ‚Äî –ø–æ–ø—Ä–æ—Å–∏–ª–∞ –º–∞–º–∞ –û–∫—Å–∞–Ω—É, —Å–ª—ñ–¥ –ø–µ—Ä–µ—Ç–≤–æ—Ä–∏—Ç–∏ –Ω–∞: –ú–∞–º–∞ –ø–æ–ø—Ä–æ—Å–∏–ª–∞ –û–∫—Å–∞–Ω—É, —â–æ–± –≤–æ–Ω–∞ –ø—Ä–∏–Ω–µ—Å–ª–∞ –∫–≤—ñ—Ç–∏.",
  "encouragement": "–ù–µ –∑–∞—Å–º—É—á—É–π—Å—è! –†–æ–∑—É–º—ñ–Ω–Ω—è –ø—Ä–∞–≤–∏–ª –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—Ä—è–º–æ—ó –º–æ–≤–∏ –≤ –Ω–µ–ø—Ä—è–º—É –ø–æ—Ç—Ä–µ–±—É—î –ø—Ä–∞–∫—Ç–∏–∫–∏. –ü–µ—Ä–µ–≥–ª—è–Ω—å —â–µ —Ä–∞–∑ —Ç–µ–æ—Ä–µ—Ç–∏—á–Ω–∏–π –º–∞—Ç–µ—Ä—ñ–∞–ª —Ç–∞ —Å–ø—Ä–æ–±—É–π –≤–∏–∫–æ–Ω–∞—Ç–∏ –±—ñ–ª—å—à–µ –≤–ø—Ä–∞–≤. –¢–∏ –æ–±–æ–≤'—è–∑–∫–æ–≤–æ –∑–º–æ–∂–µ—à —Ü–µ –æ—Å–≤–æ—ó—Ç–∏!"
}"""

    # Example knowledge graph (from your data)
    knowledge_graph = {
        "nodes": [
            {
                "id": "56f52f5f3a71bc0ce9539f26ffb0a90f",
                "node_type": "book",
                "name": "¬´–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –º–æ–≤–∞¬ª –ø—ñ–¥—Ä—É—á–Ω–∏–∫ –¥–ª—è 9 –∫–ª–∞—Å—É –∑–∞–≥–∞–ª—å–Ω–æ–æ—Å–≤—ñ—Ç–Ω—ñ—Ö –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –∑–∞–∫–ª–∞–¥—ñ–≤ ‚Äî –ó–∞–±–æ–ª–æ—Ç–Ω–∏–π –í. –í., –ó–∞–±–æ–ª–æ—Ç–Ω–∏–π –û. –í.",
                "grade": 9,
                "discipline": "–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –º–æ–≤–∞"
            },
            {
                "id": "2969158886450936084",
                "node_type": "section",
                "title": "–ü–†–Ø–ú–ê –Ü –ù–ï–ü–†–Ø–ú–ê –ú–û–í–ê",
                "book_id": "56f52f5f3a71bc0ce9539f26ffb0a90f",
                "start_page": 26,
                "end_page": 47
            },
            {
                "id": "-6813754816033091347",
                "node_type": "topic",
                "title": "¬ß 5. –ù–µ–ø—Ä—è–º–∞ –º–æ–≤–∞. –ó–∞–º—ñ–Ω–∞ –ø—Ä—è–º–æ—ó –º–æ–≤–∏ –Ω–µ–ø—Ä—è–º–æ—é",
                "type": "theoretical",
                "summary": "–ù–µ–ø—Ä—è–º–∞ –º–æ–≤–∞ ‚Äì —Ü–µ —á—É–∂–µ –º–æ–≤–ª–µ–Ω–Ω—è, —è–∫–µ –ø–µ—Ä–µ–¥–∞—é—Ç—å –Ω–µ –¥–æ—Å–ª—ñ–≤–Ω–æ, –∞ –ª–∏—à–µ –∑—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∑–º—ñ—Å—Ç—É. –†–µ—á–µ–Ω–Ω—è –∑ –Ω–µ–ø—Ä—è–º–æ—é –º–æ–≤–æ—é –∑–∞ –±—É–¥–æ–≤–æ—é —î —Å–∫–ª–∞–¥–Ω–∏–º —Ä–µ—á–µ–Ω–Ω—è–º, —á–∞—Å—Ç–∏–Ω–∏ —è–∫–æ–≥–æ –∑'—î–¥–Ω–∞–Ω–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Å–ø–æ–ª—É—á–Ω–∏–∫—ñ–≤ –∞–±–æ —Å–ø–æ–ª—É—á–Ω–∏—Ö —Å–ª—ñ–≤ (–∑–∞–π–º–µ–Ω–Ω–∏–∫—ñ–≤, –ø—Ä–∏—Å–ª—ñ–≤–Ω–∏–∫—ñ–≤), –ø—Ä–∏ —Ü—å–æ–º—É –Ω–µ–ø—Ä—è–º—É –º–æ–≤—É –∑–∞–≤–∂–¥–∏ –Ω–∞–≤–æ–¥—è—Ç—å –ø—ñ—Å–ª—è —Å–ª—ñ–≤ –∞–≤—Ç–æ—Ä–∞. –ó–∞–º—ñ–Ω–∞ –ø—Ä—è–º–æ—ó –º–æ–≤–∏ –Ω–µ–ø—Ä—è–º–æ—é –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –ø–æ-—Ä—ñ–∑–Ω–æ–º—É –¥–ª—è —Ä–æ–∑–ø–æ–≤—ñ–¥–Ω–∏—Ö, —Å–ø–æ–Ω—É–∫–∞–ª—å–Ω–∏—Ö —Ç–∞ –ø–∏—Ç–∞–ª—å–Ω–∏—Ö —Ä–µ—á–µ–Ω—å. –ü—ñ–¥ —á–∞—Å –∑–∞–º—ñ–Ω–∏ –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç—å—Å—è –≤–∏–≥—É–∫–∏, —á–∞—Å—Ç–∫–∏, –∑—Ä—ñ–¥–∫–∞ –≤—Å—Ç–∞–≤–Ω—ñ —Å–ª–æ–≤–∞; –¥—ñ—î—Å–ª–æ–≤–∞ –≤ –Ω–∞–∫–∞–∑–æ–≤—ñ–π —Ñ–æ—Ä–º—ñ –∑–∞–º—ñ–Ω—é—é—Ç—å—Å—è —ñ–Ω—à–∏–º–∏ —Ñ–æ—Ä–º–∞–º–∏; –∑–≤–µ—Ä—Ç–∞–Ω–Ω—è –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç—å—Å—è –∞–±–æ —Å—Ç–∞—é—Ç—å —á–ª–µ–Ω–∞–º–∏ —Ä–µ—á–µ–Ω–Ω—è; –∑–∞–π–º–µ–Ω–Ω–∏–∫–∏ –ø–µ—Ä—à–æ—ó —Ç–∞ –¥—Ä—É–≥–æ—ó –æ—Å–æ–±–∏ –∑–∞–º—ñ–Ω—é—é—Ç—å—Å—è —Ñ–æ—Ä–º–∞–º–∏ —Ç—Ä–µ—Ç—å–æ—ó –æ—Å–æ–±–∏. –£ –¥–µ—è–∫–∏—Ö –≤–∏–ø–∞–¥–∫–∞—Ö –ø—Ä—è–º—É –º–æ–≤—É –Ω–µ–¥–æ—Ü—ñ–ª—å–Ω–æ –∑–∞–º—ñ–Ω—è—Ç–∏ –Ω–µ–ø—Ä—è–º–æ—é, –æ—Å–∫—ñ–ª—å–∫–∏ —Ü–µ –º–æ–∂–µ —Å–ø–æ—Ç–≤–æ—Ä–∏—Ç–∏ –∑–º—ñ—Å—Ç –≤–∏—Å–ª–æ–≤–ª–µ–Ω–Ω—è.",
                "section_id": "2969158886450936084",
                "book_id": "56f52f5f3a71bc0ce9539f26ffb0a90f",
                "start_page": 30.0,
                "end_page": 33.0
            },
            {
                "id": "topic_sentence_types",
                "node_type": "topic",
                "title": "–¢–∏–ø–∏ —Ä–µ—á–µ–Ω—å –∑–∞ –º–µ—Ç–æ—é –≤–∏—Å–ª–æ–≤–ª—é–≤–∞–Ω–Ω—è",
                "type": "theoretical",
                "summary": "–†–µ—á–µ–Ω–Ω—è –ø–æ–¥—ñ–ª—è—é—Ç—å—Å—è –Ω–∞ —Ä–æ–∑–ø–æ–≤—ñ–¥–Ω—ñ, –ø–∏—Ç–∞–ª—å–Ω—ñ —Ç–∞ —Å–ø–æ–Ω—É–∫–∞–ª—å–Ω—ñ –∑–∞ –º–µ—Ç–æ—é –≤–∏—Å–ª–æ–≤–ª—é–≤–∞–Ω–Ω—è. –°–ø–æ–Ω—É–∫–∞–ª—å–Ω—ñ —Ä–µ—á–µ–Ω–Ω—è –≤–∏—Ä–∞–∂–∞—é—Ç—å –Ω–∞–∫–∞–∑, –ø—Ä–æ—Ö–∞–Ω–Ω—è, –ø–æ–±–∞–∂–∞–Ω–Ω—è.",
                "section_id": "section_syntax",
                "book_id": "56f52f5f3a71bc0ce9539f26ffb0a90f"
            },
            {
                "id": "topic_conjunctions",
                "node_type": "topic",
                "title": "–°–ø–æ–ª—É—á–Ω–∏–∫–∏ –≤ —Å–∫–ª–∞–¥–Ω–æ–ø—ñ–¥—Ä—è–¥–Ω–∏—Ö —Ä–µ—á–µ–Ω–Ω—è—Ö",
                "type": "theoretical",
                "summary": "–°–ø–æ–ª—É—á–Ω–∏–∫–∏ —Å–ª—É–∂–∞—Ç—å –¥–ª—è –∑'—î–¥–Ω–∞–Ω–Ω—è —á–∞—Å—Ç–∏–Ω —Å–∫–ª–∞–¥–Ω–æ–ø—ñ–¥—Ä—è–¥–Ω–æ–≥–æ —Ä–µ—á–µ–Ω–Ω—è. –°–ø–æ–ª—É—á–Ω–∏–∫ '—â–æ–±' –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –≤–∏—Ä–∞–∂–µ–Ω–Ω—è –º–µ—Ç–∏.",
                "book_id": "56f52f5f3a71bc0ce9539f26ffb0a90f"
            }
        ],
        "edges": [
            {
                "source": "topic:-6813754816033091347",
                "target": "term:–Ω–µ–ø—Ä—è–º–∞ –º–æ–≤–∞",
                "relation": "MENTIONS_TERM",
                "confidence": 0.95
            },
            {
                "source": "topic:-6813754816033091347",
                "target": "term:—Å–ø–æ–Ω—É–∫–∞–ª—å–Ω–µ —Ä–µ—á–µ–Ω–Ω—è",
                "relation": "MENTIONS_TERM",
                "confidence": 0.85
            },
            {
                "source": "topic:-6813754816033091347",
                "target": "term:–ø—Ä—è–º–∞ –º–æ–≤–∞",
                "relation": "MENTIONS_TERM",
                "confidence": 0.9
            },
            {
                "source": "2969158886450936084",
                "target": "topic:-6813754816033091347",
                "relation": "CONTAINS_TOPIC"
            },
            {
                "source": "56f52f5f3a71bc0ce9539f26ffb0a90f",
                "target": "2969158886450936084",
                "relation": "CONTAINS_SECTION"
            }
        ]
    }

    user_prompt = "–ü–∏—Ç–∞–Ω–Ω—è: –Ø–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∑–∞–º—ñ–Ω–∏—Ç–∏ –ø—Ä—è–º—É –º–æ–≤—É –Ω–µ–ø—Ä—è–º–æ—é –≤ —Ä–µ—á–µ–Ω–Ω—ñ ¬´–ü—Ä–∏–Ω–µ—Å–∏, –±—É–¥—å –ª–∞—Å–∫–∞, –∫–≤—ñ—Ç–∏¬ª, ‚Äî –ø–æ–ø—Ä–æ—Å–∏–ª–∞ –º–∞–º–∞ –û–∫—Å–∞–Ω—É, –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ –ø—Ä–∞–≤–∏–ª–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó —Å–ø–æ–Ω—É–∫–∞–ª—å–Ω–∏—Ö —Ä–µ—á–µ–Ω—å?"

    # Run the analysis (with pre-generated output, embeddings enabled)
    print("\n" + "=" * 80)
    print("Running analysis with pre-generated model output...")
    print("=" * 80)
    
    result = run_analysis(
        model_output=model_output,
        knowledge_graph_json=json.dumps(knowledge_graph, ensure_ascii=False),
        user_prompt=user_prompt,
        use_embeddings=False  # Set to True if sentence-transformers is installed
    )

    # Pretty print results
    print("\nüìã Processing Steps:")
    for step in result.get("processing_steps", []):
        print(f"  ‚úì {step}")
    
    if result.get("errors"):
        print("\n‚ö†Ô∏è Errors:")
        for error in result["errors"]:
            print(f"  ‚Ä¢ {error}")
    
    recommendations = result.get("recommendations", {})
    
    if recommendations:
        print("\nüìä Student Assessment:")
        assessment = recommendations.get("student_assessment", {})
        print(f"  ‚Ä¢ Correct: {assessment.get('is_correct')}")
        print(f"  ‚Ä¢ Score: {assessment.get('score')}/100")
        print(f"  ‚Ä¢ Summary: {assessment.get('summary')}")
        
        print("\nüéØ Identified Weaknesses:")
        weaknesses = recommendations.get("identified_weaknesses", {})
        if weaknesses.get("terms"):
            print(f"  Terms: {', '.join(weaknesses['terms'])}")
        if weaknesses.get("concepts"):
            print(f"  Concepts: {weaknesses['concepts'][0][:50]}...")
        if weaknesses.get("skills"):
            print(f"  Skills: {', '.join(weaknesses['skills'])}")
        
        print("\nüìö Topic Recommendations:")
        for i, topic in enumerate(recommendations.get("topic_recommendations", [])[:5], 1):
            print(f"  {i}. {topic.get('suggested')}")
            print(f"     ‚Üí Matched: {topic.get('curriculum_topic')}")
            print(f"     ‚Üí Score: {topic.get('combined_score', 0):.2f}")
        
        print("\nüìñ Learning Path:")
        for item in recommendations.get("learning_path", []):
            print(f"  ‚Ä¢ {item.get('topic')}")
            if item.get('section'):
                print(f"    Section: {item['section']}")
            if item.get('pages'):
                print(f"    Pages: {item['pages']}")
        
        print(f"\nüí¨ Encouragement: {assessment.get('encouragement')}")
    
    print("\n" + "=" * 80)
    
    # Return full result for programmatic use
    return result


def run_with_vllm(
    question: str,
    correct_answer: str,
    student_answer: str,
    student_explanation: str = "",
    knowledge_graph: Optional[Dict] = None,
    vllm_host: str = "localhost",
    vllm_port: int = 8000
) -> Dict[str, Any]:
    """
    Run analysis using local vLLM model
    
    Example:
        result = run_with_vllm(
            question="–Ø–∫ –∑–∞–º—ñ–Ω–∏—Ç–∏ –ø—Ä—è–º—É –º–æ–≤—É –Ω–µ–ø—Ä—è–º–æ—é?",
            correct_answer="–ú–∞–º–∞ –ø–æ–ø—Ä–æ—Å–∏–ª–∞ –û–∫—Å–∞–Ω—É, —â–æ–± –≤–æ–Ω–∞ –ø—Ä–∏–Ω–µ—Å–ª–∞ –∫–≤—ñ—Ç–∏.",
            student_answer="–ù–µ –∑–Ω–∞—é",
            knowledge_graph=my_kg
        )
    """
    return run_analysis(
        use_vllm=True,
        question=question,
        correct_answer=correct_answer,
        student_answer=student_answer,
        student_explanation=student_explanation,
        knowledge_graph_json=json.dumps(knowledge_graph) if knowledge_graph else None,
        vllm_host=vllm_host,
        vllm_port=vllm_port
    )


if __name__ == "__main__":
    result = main()
    
    # Optionally save results to file
    with open("analysis_results.json", "w", encoding="utf-8") as f:
        # Convert non-serializable objects
        output = {
            "recommendations": result.get("recommendations"),
            "processing_steps": result.get("processing_steps"),
            "errors": result.get("errors"),
            "analysis_result": result.get("analysis_result"),
            "topic_matches": result.get("topic_matches")
        }
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print("\n‚úÖ Results saved to analysis_results.json")
    
    # Print vLLM command
    print("\n" + "=" * 80)
    print("To use with your local model, run vLLM with:")
    print("=" * 80)
    print(get_vllm_launch_command())