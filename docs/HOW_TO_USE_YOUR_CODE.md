# üéØ –ö–∞–∫ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–∞—à –∫–æ–¥ –∏–∑ Lapathon3.zip –ø–æ–¥ –∑–∞–¥–∞—á—É —Ö–∞–∫–∞—Ç–æ–Ω–∞

## ‚ùå –ß—Ç–æ —è —Å–¥–µ–ª–∞–ª –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û

–Ø —Å–æ–∑–¥–∞–ª —Ä–µ—à–µ–Ω–∏–µ "—Å –Ω—É–ª—è", –ù–ï –∏—Å–ø–æ–ª—å–∑—É—è –≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –∏–∑ `Lapathon3.zip`.

## ‚úÖ –ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ü–†–ê–í–ò–õ–¨–ù–û

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–í–ê–® –ö–û–î** –∫–∞–∫ –æ—Å–Ω–æ–≤—É –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –ø–æ–¥ –¥–≤–∞ endpoint'–∞ —Ö–∞–∫–∞—Ç–æ–Ω–∞.

---

## üì¶ –ß—Ç–æ —É –≤–∞—Å –£–ñ–ï–ù–ï–°–¢–¨ –≤ –∞—Ä—Ö–∏–≤–µ

### 1. `agent.py` - –ö–ª—é—á–µ–≤–æ–π —Ñ–∞–π–ª —Å LLM –≤—ã–∑–æ–≤–∞–º–∏
**–ß—Ç–æ —Ç–∞–º –µ—Å—Ç—å:**
- ‚úÖ –ù–∞—Å—Ç–æ—è—â–∏–µ –≤—ã–∑–æ–≤—ã Lapa LLM —á–µ—Ä–µ–∑ `langchain_openai.ChatOpenAI`
- ‚úÖ RAG —Ñ—É–Ω–∫—Ü–∏—è `_retrieve_context()` —Å –ø–æ–∏—Å–∫–æ–º –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
- ‚úÖ LangGraph –ø–∞–π–ø–ª–∞–π–Ω —Å nodes
- ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ Parquet –¥–∞–Ω–Ω—ã—Ö
- ‚úÖ –†–∞–±–æ—Ç–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏

**–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
```python
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏—Ö LLM –∫–ª–∏–µ–Ω—Ç:
from agent import _get_llm_reasoning, _get_llm_classification

llm = _get_llm_reasoning()
response = llm.invoke([
    SystemMessage(content="–¢–∏ - –≤—á–∏—Ç–µ–ª—å..."),
    HumanMessage(content="–°—Ç–≤–æ—Ä–∏ –º–∞—Ç–µ—Ä—ñ–∞–ª...")
])
```

### 2. `hybrid_retriever.py` - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π RAG
**–ß—Ç–æ —Ç–∞–º –µ—Å—Ç—å:**
- ‚úÖ FAISS –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
- ‚úÖ BM25 sparse retrieval
- ‚úÖ Reranking —Å cross-encoder
- ‚úÖ Hybrid fusion (dense + sparse)

**–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
```python
from hybrid_retriever import HybridRetriever, HybridConfig

retriever = HybridRetriever(config=HybridConfig())
retriever.load_data(toc_df, pages_df)
retriever.build_indexes()

results = retriever.retrieve(query="–∫–≤–∞–¥—Ä–∞—Ç–Ω—ñ —Ä—ñ–≤–Ω—è–Ω–Ω—è", top_k=5)
```

### 3. –î—Ä—É–≥–∏–µ –ø–æ–ª–µ–∑–Ω—ã–µ —Ñ–∞–π–ª—ã:
- `eval.py` - –ø—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã —Å questions
- `test_hybrid_retriever.py` - –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- `data_analysis.py` - –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö

---

## üîß –ü–ª–∞–Ω –∞–¥–∞–ø—Ç–∞—Ü–∏–∏

### –®–ê–ì 1: –°–æ–∑–¥–∞–π—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞

```bash
mriia_hackathon/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ agent.py              # ‚Üê –ö–æ–ø–∏—Ä—É–µ–º –∏–∑ –∞—Ä—Ö–∏–≤–∞
‚îÇ   ‚îú‚îÄ‚îÄ hybrid_retriever.py   # ‚Üê –ö–æ–ø–∏—Ä—É–µ–º –∏–∑ –∞—Ä—Ö–∏–≤–∞
‚îÇ   ‚îú‚îÄ‚îÄ api_server.py         # ‚Üê –ù–û–í–´–ô: FastAPI endpoints
‚îÇ   ‚îú‚îÄ‚îÄ tutor_service.py      # ‚Üê –ù–û–í–´–ô: –õ–æ–≥–∏–∫–∞ —Ç—å—é—Ç–æ—Ä–∞
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îî‚îÄ‚îÄ app.py               # Streamlit UI
‚îî‚îÄ‚îÄ data/                    # Parquet —Ñ–∞–π–ª—ã
```

### –®–ê–ì 2: –°–æ–∑–¥–∞–π—Ç–µ API Server (api_server.py)

```python
"""
FastAPI —Å–µ—Ä–≤–µ—Ä –∏—Å–ø–æ–ª—å–∑—É—è –í–ê–® –ö–û–î –∏–∑ agent.py –∏ hybrid_retriever.py
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict
import pandas as pd

# –ò–ú–ü–û–†–¢–ò–†–£–ï–ú –í–ê–® –ö–û–î
from agent import _get_llm_reasoning, _load_rag_data, _retrieve_context
from hybrid_retriever import HybridRetriever, HybridConfig

app = FastAPI(title="Mriia AI Tutor - Based on Your Code")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
retriever = None
llm_client = None

@app.on_event("startup")
async def startup():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É—è –≤–∞—à –∫–æ–¥"""
    global retriever, llm_client
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –í–ê–®–ò–ú —Å–ø–æ—Å–æ–±–æ–º
    rag_data = _load_rag_data()
    
    # 2. –°–æ–∑–¥–∞–µ–º retriever –∏–∑ –í–ê–®–ï–ì–û hybrid_retriever.py
    retriever = HybridRetriever(config=HybridConfig(
        final_top_k=5,
        use_reranking=True
    ))
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ retriever
    if "toc_gemini" in rag_data:
        toc_df = rag_data["toc_gemini"]
        retriever.load_toc_data(toc_df)
    
    if "pages_gemini" in rag_data:
        pages_df = rag_data["pages_gemini"]
        retriever.load_pages_data(pages_df)
    
    retriever.build_indexes()
    
    # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º LLM –í–ê–®–ò–ú —Å–ø–æ—Å–æ–±–æ–º
    llm_client = _get_llm_reasoning()
    
    print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É—è –≤–∞—à –∫–æ–¥!")


class GenerateRequest(BaseModel):
    topic: str
    grade: int
    subject: str


@app.post("/api/generate-material")
async def generate_material(request: GenerateRequest):
    """
    Endpoint 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –í–ê–® retriever –∏ LLM client
    """
    
    # 1. RAG - –∏—Å–ø–æ–ª—å–∑—É–µ–º –í–ê–®–£ —Ñ—É–Ω–∫—Ü–∏—é
    context = _retrieve_context(
        question_text=request.topic,
        subject=request.subject,
        top_k=3
    )
    
    # –ò–õ–ò –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞—à hybrid retriever
    # results = retriever.retrieve(query=request.topic, top_k=5)
    # context = "\n".join([r.content for r in results])
    
    # 2. LLM –≥–µ–Ω–µ—Ä–∞—Ü–∏—è - –∏—Å–ø–æ–ª—å–∑—É–µ–º –í–ê–®–ï–ì–û –∫–ª–∏–µ–Ω—Ç–∞
    from langchain_core.messages import SystemMessage, HumanMessage
    
    messages = [
        SystemMessage(content=f"–¢–∏ - –≤—á–∏—Ç–µ–ª—å –¥–ª—è {request.grade} –∫–ª–∞—Å—É"),
        HumanMessage(content=f"""–°—Ç–≤–æ—Ä–∏ –Ω–∞–≤—á–∞–ª—å–Ω–∏–π –º–∞—Ç–µ—Ä—ñ–∞–ª:
        
–¢–µ–º–∞: {request.topic}
–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context[:1000]}

–°—Ç–≤–æ—Ä–∏:
1. –ö–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Å–ø–µ–∫—Ç
2. –î–µ—Ç–∞–ª—å–Ω–µ –ø–æ—è—Å–Ω–µ–Ω–Ω—è
3. 10 —Ç–µ—Å—Ç–æ–≤–∏—Ö –ø–∏—Ç–∞–Ω—å

–§–æ—Ä–º–∞—Ç:
–ö–û–ù–°–ü–ï–ö–¢: ...
–ü–û–Ø–°–ù–ï–ù–ù–Ø: ...
–¢–ï–°–¢–ò: ...
""")
    ]
    
    response = llm_client.invoke(messages)
    content = response.content
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∏ –≤–æ–∑–≤—Ä–∞—Ç
    return {
        "summary": content,  # –ü–∞—Ä—Å–∏—Ç–µ —ç—Ç–æ
        "exercises": [],     # –ü–∞—Ä—Å–∏—Ç–µ tests –∏–∑ content
        "context_used": context[:500]
    }


# –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è /api/assess-student
```

### –®–ê–ì 3: –°–æ–∑–¥–∞–π—Ç–µ Tutor Service (tutor_service.py)

```python
"""
–°–µ—Ä–≤–∏—Å —Ç—å—é—Ç–æ—Ä–∞ –Ω–∞ –±–∞–∑–µ –í–ê–®–ï–ì–û –∫–æ–¥–∞
"""

from agent import _get_llm_reasoning, _retrieve_context
from hybrid_retriever import HybridRetriever
from langchain_core.messages import SystemMessage, HumanMessage

class TutorService:
    """–û–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ –≤–∞—à–∏–º –∫–æ–¥–æ–º –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞"""
    
    def __init__(self, retriever: HybridRetriever):
        self.retriever = retriever
        self.llm = _get_llm_reasoning()
    
    def generate_content(self, topic: str, grade: int, subject: str):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑—É—è:
        1. –í–∞—à retriever –¥–ª—è RAG
        2. –í–∞—à LLM client –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        
        # RAG - –≤–∞—à –∫–æ–¥
        context = _retrieve_context(topic, subject)
        
        # LLM - –≤–∞—à –∫–æ–¥
        messages = [
            SystemMessage(content=f"–í—á–∏—Ç–µ–ª—å –¥–ª—è {grade} –∫–ª–∞—Å—É"),
            HumanMessage(content=f"–¢–µ–º–∞: {topic}\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}")
        ]
        
        response = self.llm.invoke(messages)
        
        return self._parse_response(response.content)
    
    def generate_exercises(self, topic: str, count: int = 10):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ—Å—Ç—ã —á–µ—Ä–µ–∑ –≤–∞—à LLM"""
        messages = [
            SystemMessage(content="–°—Ç–≤–æ—Ä–∏ —Ç–µ—Å—Ç–æ–≤—ñ –∑–∞–≤–¥–∞–Ω–Ω—è"),
            HumanMessage(content=f"–¢–µ–º–∞: {topic}, –∫—ñ–ª—å–∫—ñ—Å—Ç—å: {count}")
        ]
        
        response = self.llm.invoke(messages)
        return self._parse_exercises(response.content)
    
    def assess_student(self, answers, exercises):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —á–µ—Ä–µ–∑ –≤–∞—à LLM"""
        # –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–∞—à LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        pass
```

### –®–ê–ì 4: Requirements.txt –∏–∑ –í–ê–®–ï–ì–û –∫–æ–¥–∞

```txt
# –û—Å–Ω–æ–≤–Ω—ã–µ (–∏–∑ –≤–∞—à–µ–≥–æ pyproject.toml)
langchain==1.2.4
langchain-openai==1.1.7
langchain-core==1.2.7
langgraph==1.0.6

# RAG (–∏–∑ –≤–∞—à–µ–≥–æ hybrid_retriever.py)
faiss-cpu==1.7.4
sentence-transformers==5.2.0
rank-bm25==0.2.2

# Data
pandas==2.3.3
pyarrow==22.0.0
numpy==2.4.1

# FastAPI (–Ω–æ–≤–æ–µ –¥–ª—è endpoints)
fastapi==0.128.0
uvicorn==0.40.0
pydantic==2.12.5
```

---

## üéØ –û—Å–Ω–æ–≤–Ω—ã–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏

### –ê–¥–∞–ø—Ç–∞—Ü–∏—è 1: RAG —Ñ—É–Ω–∫—Ü–∏—è

**–í–ê–®–ê —Ñ—É–Ω–∫—Ü–∏—è** (`agent.py:155`):
```python
def _retrieve_context(question_text: str, subject: str, top_k: int = RAG_TOP_K) -> str:
    # ... –≤–∞—à –∫–æ–¥ –ø–æ–∏—Å–∫–∞ –ø–æ text similarity
```

**–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è endpoint:**
```python
@app.post("/api/generate-material")
async def generate_material(request):
    # –ü—Ä–æ—Å—Ç–æ –≤—ã–∑—ã–≤–∞–µ–º –≤–∞—à—É —Ñ—É–Ω–∫—Ü–∏—é!
    context = _retrieve_context(
        question_text=request.topic,
        subject=request.subject
    )
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º context –¥–ª—è LLM
```

### –ê–¥–∞–ø—Ç–∞—Ü–∏—è 2: LLM Client

**–í–ê–® LLM client** (`agent.py:254`):
```python
def _get_llm_reasoning() -> ChatOpenAI:
    return ChatOpenAI(
        model="lapa",
        api_key=LLM_API_KEY,
        base_url=LLM_API_BASE,
        temperature=0.1,
        max_tokens=512
    )
```

**–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
```python
llm = _get_llm_reasoning()

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
response = llm.invoke([
    SystemMessage(content="System prompt..."),
    HumanMessage(content="User query...")
])

content = response.content  # –û—Ç–≤–µ—Ç LLM
```

### –ê–¥–∞–ø—Ç–∞—Ü–∏—è 3: Hybrid Retriever

**–í–ê–® retriever** (`hybrid_retriever.py`):
```python
from hybrid_retriever import HybridRetriever, HybridConfig

retriever = HybridRetriever(config=HybridConfig())
retriever.load_data(toc_df=toc, pages_df=pages)
retriever.build_indexes()

results = retriever.retrieve(query="–∫–≤–∞–¥—Ä–∞—Ç–Ω—ñ —Ä—ñ–≤–Ω—è–Ω–Ω—è")
```

**–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤ endpoint:**
```python
@app.post("/api/generate-material")
async def generate_material(request):
    # –í–ê–®–ò retrieval results
    results = retriever.retrieve(
        query=request.topic,
        top_k=5,
        filters={"grade": request.grade}
    )
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º context
    context = "\n\n".join([r.content for r in results])
    
    # –ü–µ—Ä–µ–¥–∞–µ–º –≤ LLM
```

---

## üìù –ò—Ç–æ–≥–æ–≤—ã–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥

### –§–∞–π–ª—ã –∫–æ—Ç–æ—Ä—ã–µ –±–µ—Ä–µ–º –ò–ó –í–ê–®–ï–ì–û –ê–†–•–ò–í–ê:
1. ‚úÖ `agent.py` - LLM clients –∏ RAG
2. ‚úÖ `hybrid_retriever.py` - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π retrieval
3. ‚úÖ `requirements.txt` / `pyproject.toml` - Dependencies

### –§–∞–π–ª—ã –∫–æ—Ç–æ—Ä—ã–µ –°–û–ó–î–ê–ï–ú –ù–û–í–´–ï:
1. ‚ûï `api_server.py` - FastAPI —Å endpoints
2. ‚ûï `tutor_service.py` - –û–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ –≤–∞—à–∏–º –∫–æ–¥–æ–º
3. ‚ûï `frontend/app.py` - Streamlit UI

### –õ–æ–≥–∏–∫–∞ —Ä–∞–±–æ—Ç—ã:

```
User Request ‚Üí FastAPI endpoint
                    ‚Üì
            TutorService (–Ω–æ–≤—ã–π)
                    ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì                       ‚Üì
   –í–ê–® agent.py          –í–ê–® hybrid_retriever.py
   (_retrieve_context)   (HybridRetriever)
        ‚Üì                       ‚Üì
   –í–ê–® LLM client        –í–ê–®–ò FAISS indexes
   (_get_llm_reasoning)
        ‚Üì
    Response
```

---

## ‚úÖ –ß–µ–∫–ª–∏—Å—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

- [ ] –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å `agent.py` –∏ `hybrid_retriever.py` –∏–∑ –∞—Ä—Ö–∏–≤–∞
- [ ] –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å `_get_llm_reasoning()` –¥–ª—è LLM –≤—ã–∑–æ–≤–æ–≤
- [ ] –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å `_retrieve_context()` –¥–ª—è RAG
- [ ] –°–æ–∑–¥–∞—Ç—å FastAPI –æ–±–µ—Ä—Ç–∫—É –≤–æ–∫—Ä—É–≥ –≤–∞—à–µ–≥–æ –∫–æ–¥–∞
- [ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∞—à–∏ requirements (langchain, faiss, etc)
- [ ] –ù–ï –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å RAG –ª–æ–≥–∏–∫—É - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∞—à—É
- [ ] –ù–ï –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å LLM –≤—ã–∑–æ–≤—ã - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∞—à–∏

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç —Å –í–ê–®–ò–ú –∫–æ–¥–æ–º

```bash
# 1. –†–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å –≤–∞—à –∞—Ä—Ö–∏–≤
unzip Lapathon3.zip -d lapathon_base

# 2. –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–µ–∫—Ç
mkdir -p mriia_hackathon/backend
cd mriia_hackathon/backend

# 3. –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –í–ê–®–ò —Ñ–∞–π–ª—ã
cp ../../lapathon_base/agent.py .
cp ../../lapathon_base/hybrid_retriever.py .

# 4. –°–æ–∑–¥–∞—Ç—å FastAPI –æ–±–µ—Ä—Ç–∫—É (api_server.py)
# –ò—Å–ø–æ–ª—å–∑—É—è –ø—Ä–∏–º–µ—Ä—ã –≤—ã—à–µ

# 5. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install -r requirements.txt  # –ò–∑ –≤–∞—à–µ–≥–æ –∞—Ä—Ö–∏–≤–∞

# 6. –ó–∞–ø—É—Å—Ç–∏—Ç—å
python api_server.py
```

---

## üí° –ü–æ—á–µ–º—É —ç—Ç–æ –ü–†–ê–í–ò–õ–¨–ù–´–ô –ø–æ–¥—Ö–æ–¥

1. **–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –í–ê–®–ê —Ä–∞–±–æ—Ç–∞** - agent.py, hybrid_retriever.py
2. **–ù–µ –∏–∑–æ–±—Ä–µ—Ç–∞–µ—Ç—Å—è –≤–µ–ª–æ—Å–∏–ø–µ–¥** - RAG –∏ LLM —É–∂–µ –µ—Å—Ç—å
3. **–ú–∏–Ω–∏–º—É–º –Ω–æ–≤–æ–≥–æ –∫–æ–¥–∞** - —Ç–æ–ª—å–∫–æ FastAPI –æ–±–µ—Ä—Ç–∫–∞
4. **–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ** - –≤–∞—à –∫–æ–¥ —É–∂–µ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω
5. **–õ–µ–≥–∫–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å** - –ø–æ–Ω—è—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞

---

**–í–´–í–û–î:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –∫–∞–∫ –±–∏–±–ª–∏–æ—Ç–µ–∫—É, –¥–æ–±–∞–≤—å—Ç–µ —Ç–æ–ª—å–∫–æ FastAPI endpoints!
